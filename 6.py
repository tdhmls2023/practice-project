import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image, ImageOps
import os
import seaborn as sns

# è¯„ä¼°æŒ‡æ ‡ï¼šæ··æ·†çŸ©é˜µã€åˆ†ç±»æŠ¥å‘Šï¼ˆç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ï¼‰
from sklearn.metrics import confusion_matrix, classification_report

# PyTorchç¥ç»ç½‘ç»œæ¨¡å—ï¼šå®šä¹‰å±‚ã€æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim  # ä¼˜åŒ–å™¨ï¼ˆAdamï¼‰
from torch.optim.lr_scheduler import CosineAnnealingLR  # å­¦ä¹ ç‡è°ƒåº¦å™¨
from torchsummary import summary  # æ¨¡å‹ç»“æ„å¯è§†åŒ–ï¼ˆè¾“å‡ºå±‚ç»´åº¦ã€å‚æ•°ï¼‰

import warnings

warnings.filterwarnings('ignore')  # å¿½ç•¥æ— å…³è­¦å‘Šï¼ˆå¦‚æ•°æ®åŠ è½½æç¤ºï¼‰

# è®¾ç½®Matplotlibä¸­æ–‡æ˜¾ç¤ºï¼ˆè§£å†³ä¸­æ–‡ä¹±ç ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# è‡ªåŠ¨é€‰æ‹©è®¡ç®—è®¾å¤‡ï¼šä¼˜å…ˆä½¿ç”¨GPUï¼ˆCUDAï¼‰ï¼Œæ— åˆ™ç”¨CPU
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")  # æ‰“å°å½“å‰ä½¿ç”¨çš„è®¾å¤‡ï¼ˆä¾¿äºè°ƒè¯•ï¼‰


class DataProcessor:

    def __init__(self, batch_size=64):
        """
        åˆå§‹åŒ–å‚æ•°
        :param batch_size: æ¯æ¬¡è¿­ä»£åŠ è½½çš„æ ·æœ¬æ•°ï¼ˆæ‰¹é‡å¤§å°ï¼‰ï¼Œé»˜è®¤64
        """
        self.batch_size = batch_size  # æ‰¹é‡å¤§å°ï¼ˆå½±å“è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ï¼‰
        # ç±»åˆ«åç§°ï¼ˆä¸­æ–‡æ ‡æ³¨ï¼Œä¾¿äºåˆ†æï¼‰ï¼šå¯¹åº”Fashion-MNISTçš„10ä¸ªæœè£…ç±»åˆ«
        self.classes = ['Tæ¤/ä¸Šè¡£', 'è£¤å­', 'å¥—å¤´è¡«', 'è¿è¡£è£™', 'å¤–å¥—',
                        'å‡‰é‹', 'è¡¬è¡«', 'è¿åŠ¨é‹', 'åŒ…', 'çŸ­é´']

        # å›¾åƒé¢„å¤„ç†æµæ°´çº¿ï¼šå°†PILå›¾ç‰‡â†’å¼ é‡â†’æ ‡å‡†åŒ–
        self.transform = transforms.Compose([
            transforms.ToTensor(),  # å°†PILå›¾ç‰‡è½¬æ¢ä¸ºå¼ é‡ï¼ˆèŒƒå›´0-1ï¼‰
            transforms.Normalize((0.5,), (0.5,))  # æ ‡å‡†åŒ–ï¼š(x-0.5)/0.5 â†’ èŒƒå›´-1åˆ°1
        ])

        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨å’Œæ•°æ®é›†ï¼ˆåç»­load_dataæ–¹æ³•èµ‹å€¼ï¼‰
        self.trainloader = None  # è®­ç»ƒé›†åŠ è½½å™¨
        self.testloader = None  # æµ‹è¯•é›†åŠ è½½å™¨
        self.trainset = None  # è®­ç»ƒé›†ï¼ˆåŸå§‹æ•°æ®ï¼‰
        self.testset = None  # æµ‹è¯•é›†ï¼ˆåŸå§‹æ•°æ®ï¼‰

    def load_data(self):
        """
        åŠ è½½Fashion-MNISTæ•°æ®é›†å¹¶åˆ›å»ºæ•°æ®åŠ è½½å™¨
        :return: trainloader, testloader - è®­ç»ƒ/æµ‹è¯•é›†åŠ è½½å™¨
        """
        # åŠ è½½è®­ç»ƒé›†ï¼šroot=æ•°æ®ä¿å­˜è·¯å¾„ï¼Œtrain=True=è®­ç»ƒé›†ï¼Œdownload=True=è‡ªåŠ¨ä¸‹è½½
        self.trainset = torchvision.datasets.FashionMNIST(
            root='./data', train=True, download=True, transform=self.transform)
        # åŠ è½½æµ‹è¯•é›†ï¼štrain=False=æµ‹è¯•é›†
        self.testset = torchvision.datasets.FashionMNIST(
            root='./data', train=False, download=True, transform=self.transform)

        # åˆ›å»ºè®­ç»ƒé›†åŠ è½½å™¨ï¼šshuffle=True=æ‰“ä¹±æ•°æ®ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰ï¼Œnum_workers=0=å•è¿›ç¨‹ï¼ˆWindowså…¼å®¹ï¼‰
        self.trainloader = DataLoader(
            self.trainset, batch_size=self.batch_size, shuffle=True, num_workers=0)
        # åˆ›å»ºæµ‹è¯•é›†åŠ è½½å™¨ï¼šshuffle=False=ä¸æ‰“ä¹±ï¼ˆè¯„ä¼°æ—¶æ— éœ€æ‰“ä¹±ï¼‰
        self.testloader = DataLoader(
            self.testset, batch_size=self.batch_size, shuffle=False, num_workers=0)

        return self.trainloader, self.testloader

    def visualize_random_samples(self):
        """
        éšæœºå±•ç¤ºæ¯ä¸ªç±»åˆ«çš„æ ·æœ¬å›¾åƒï¼ˆç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡ä¿å­˜ï¼‰
        """
        # åˆ›å»º2è¡Œ5åˆ—çš„å­å›¾ï¼ˆ10ä¸ªç±»åˆ«ï¼‰ï¼Œè®¾ç½®ç”»å¸ƒå¤§å°
        fig, axes = plt.subplots(2, 5, figsize=(15, 8))
        fig.suptitle('Fashion-MNIST æ¯ä¸ªç±»åˆ«éšæœºæ ·æœ¬', fontsize=16)  # æ€»æ ‡é¢˜

        # éå†10ä¸ªç±»åˆ«
        for i in range(10):
            # æ‰¾åˆ°å½“å‰ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•
            class_indices = np.where(self.trainset.targets.numpy() == i)[0]
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬ç´¢å¼•ï¼ˆé¿å…å›ºå®šå±•ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
            random_idx = np.random.choice(class_indices)
            img, label = self.trainset[random_idx]  # è·å–æ ·æœ¬å›¾ç‰‡å’Œæ ‡ç­¾

            # åå½’ä¸€åŒ–ï¼šå°†æ ‡å‡†åŒ–åçš„å¼ é‡ï¼ˆ-1~1ï¼‰è½¬å›0~1ï¼Œä¾¿äºå¯è§†åŒ–
            img = img.squeeze().numpy() * 0.5 + 0.5

            # é€‰æ‹©å¯¹åº”çš„å­å›¾ï¼ˆi//5=è¡Œç´¢å¼•ï¼Œi%5=åˆ—ç´¢å¼•ï¼‰
            ax = axes[i // 5, i % 5]
            ax.imshow(img, cmap='gray')  # ä»¥ç°åº¦å›¾æ˜¾ç¤º
            ax.set_title(f'ç±»åˆ« {i}: {self.classes[label]}', fontsize=12)  # å­å›¾æ ‡é¢˜ï¼ˆç±»åˆ«+åç§°ï¼‰
            ax.axis('off')  # å…³é—­åæ ‡è½´ï¼ˆæ›´ç¾è§‚ï¼‰

        plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å­å›¾é—´è·
        # ä¿å­˜å›¾ç‰‡ï¼šdpi=150=é«˜æ¸…ï¼Œbbox_inches='tight'=è£å‰ªå¤šä½™ç©ºç™½
        plt.savefig('random_class_samples.png', dpi=150, bbox_inches='tight')
        plt.close()  # å…³é—­ç”»å¸ƒï¼ˆé‡Šæ”¾å†…å­˜ï¼‰
        print("âœ… å·²ä¿å­˜æ¯ä¸ªç±»åˆ«çš„éšæœºæ ·æœ¬å›¾åƒ: random_class_samples.png")


class FashionCNN(nn.Module):
    """
    å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹ç±»ï¼šç”¨äºæœè£…å›¾åƒåˆ†ç±»
    ç»“æ„ï¼š3ä¸ªå·ç§¯å—ï¼ˆå·ç§¯+æ‰¹å½’ä¸€åŒ–+æ¿€æ´»+æ± åŒ–ï¼‰ + 3ä¸ªå…¨è¿æ¥å±‚
    è¾“å…¥ï¼š1Ã—28Ã—28ç°åº¦å›¾ â†’ è¾“å‡ºï¼š10ä¸ªç±»åˆ«çš„æ¦‚ç‡
    """

    def __init__(self):
        super(FashionCNN, self).__init__()  # ç»§æ‰¿nn.Moduleçš„åˆå§‹åŒ–

        # å·ç§¯å—1ï¼šè¾“å…¥1Ã—28Ã—28 â†’ è¾“å‡º32Ã—14Ã—14
        self.conv_block1 = nn.Sequential(
            # å·ç§¯å±‚ï¼šin_channels=1ï¼ˆè¾“å…¥é€šé“ï¼‰ï¼Œout_channels=32ï¼ˆè¾“å‡ºé€šé“/å·ç§¯æ ¸æ•°ï¼‰ï¼Œkernel_size=3ï¼ˆå·ç§¯æ ¸å¤§å°ï¼‰ï¼Œpadding=1ï¼ˆè¾¹ç¼˜å¡«å……1åœˆ0ï¼Œä¿æŒå°ºå¯¸ï¼‰
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),  # æ‰¹é‡å½’ä¸€åŒ–ï¼šåŠ é€Ÿè®­ç»ƒï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
            nn.ReLU(inplace=True),  # ReLUæ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§ï¼Œinplace=True=åŸåœ°è¿ç®—ï¼ˆèŠ‚çœå†…å­˜ï¼‰
            nn.MaxPool2d(2, 2)  # æœ€å¤§æ± åŒ–ï¼šæ ¸å¤§å°2Ã—2ï¼Œæ­¥é•¿2 â†’ å°ºå¯¸å‡åŠï¼ˆ28â†’14ï¼‰
        )

        # å·ç§¯å—2ï¼šè¾“å…¥32Ã—14Ã—14 â†’ è¾“å‡º64Ã—7Ã—7
        self.conv_block2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # è¾“å…¥32é€šé“â†’è¾“å‡º64é€šé“
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)  # å°ºå¯¸å‡åŠï¼ˆ14â†’7ï¼‰
        )

        # å·ç§¯å—3ï¼šè¾“å…¥64Ã—7Ã—7 â†’ è¾“å‡º128Ã—3Ã—3
        self.conv_block3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # è¾“å…¥64é€šé“â†’è¾“å‡º128é€šé“
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)  # å°ºå¯¸å‡åŠï¼ˆ7â†’3ï¼Œå‘ä¸‹å–æ•´ï¼‰
        )

        # å…¨è¿æ¥å±‚ï¼šè¾“å…¥128Ã—3Ã—3ï¼ˆå±•å¹³åï¼‰ â†’ è¾“å‡º10ï¼ˆç±»åˆ«æ•°ï¼‰
        self.fc_layers = nn.Sequential(
            nn.Linear(128 * 3 * 3, 256),  # å±•å¹³ï¼š128Ã—3Ã—3=1152 â†’ 256ç»´
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),  # Dropoutï¼šéšæœºå¤±æ´»30%ç¥ç»å…ƒï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
            nn.Linear(256, 128),  # 256 â†’ 128ç»´
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),  # å†æ¬¡Dropoutå¢å¼ºæ³›åŒ–èƒ½åŠ›
            nn.Linear(128, 10)  # 128 â†’ 10ç»´ï¼ˆå¯¹åº”10ä¸ªç±»åˆ«ï¼‰
        )

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šå®šä¹‰æ•°æ®åœ¨æ¨¡å‹ä¸­çš„æµåŠ¨è·¯å¾„
        :param x: è¾“å…¥å¼ é‡ï¼ˆbatch_size Ã— 1 Ã— 28 Ã— 28ï¼‰
        :return: è¾“å‡ºå¼ é‡ï¼ˆbatch_size Ã— 10ï¼‰
        """
        x = self.conv_block1(x)  # å·ç§¯å—1å¤„ç†
        x = self.conv_block2(x)  # å·ç§¯å—2å¤„ç†
        x = self.conv_block3(x)  # å·ç§¯å—3å¤„ç†
        x = x.view(-1, 128 * 3 * 3)  # å±•å¹³ï¼š(batch_size Ã— 128 Ã— 3 Ã— 3) â†’ (batch_size Ã— 1152)
        x = self.fc_layers(x)  # å…¨è¿æ¥å±‚å¤„ç†
        return x  # è¿”å›10ä¸ªç±»åˆ«çš„é¢„æµ‹å€¼ï¼ˆæœªå½’ä¸€åŒ–ï¼‰


class ModelTrainer:

    def __init__(self, model, trainloader, testloader):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        :param model: å¾…è®­ç»ƒçš„CNNæ¨¡å‹
        :param trainloader: è®­ç»ƒé›†åŠ è½½å™¨
        :param testloader: æµ‹è¯•é›†åŠ è½½å™¨
        """
        self.model = model.to(DEVICE)  # å°†æ¨¡å‹ç§»åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰
        self.trainloader = trainloader
        self.testloader = testloader
        self.criterion = nn.CrossEntropyLoss()  # æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µï¼ˆåˆ†ç±»ä»»åŠ¡ä¸“ç”¨ï¼‰
        self.optimizer = optim.Adam(model.parameters(), lr=0.001)  # ä¼˜åŒ–å™¨ï¼šAdamï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šä½™å¼¦é€€ç«ï¼ˆT_max=10â†’10è½®åå­¦ä¹ ç‡é™ä¸ºåˆå§‹å€¼çš„ä¸€åŠï¼‰
        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=10)
        self.best_accuracy = 0.0  # è®°å½•æœ€ä½³æµ‹è¯•é›†å‡†ç¡®ç‡ï¼ˆç”¨äºä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼‰
        self.final_test_acc = 0.0  # ä¿å­˜æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡ï¼ˆç”¨äºæ··æ·†çŸ©é˜µæ ‡é¢˜ï¼‰

    def train_epoch(self):
        """
        å•è½®è®­ç»ƒï¼šéå†ä¸€æ¬¡è®­ç»ƒé›†ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°
        :return: avg_loss - æœ¬è½®å¹³å‡æŸå¤±ï¼Œaccuracy - æœ¬è½®è®­ç»ƒé›†å‡†ç¡®ç‡
        """
        self.model.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨Dropoutã€BatchNormè®­ç»ƒæ¨¡å¼ï¼‰
        running_loss = 0.0  # ç´¯è®¡æŸå¤±
        correct = 0  # æ­£ç¡®é¢„æµ‹æ•°
        total = 0  # æ€»æ ·æœ¬æ•°

        # éå†è®­ç»ƒé›†æ‰¹æ¬¡
        for inputs, targets in self.trainloader:
            # å°†è¾“å…¥å’Œæ ‡ç­¾ç§»åˆ°æŒ‡å®šè®¾å¤‡
            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)

            self.optimizer.zero_grad()  # æ¢¯åº¦æ¸…é›¶ï¼ˆé¿å…ç´¯ç§¯ä¸Šä¸€è½®æ¢¯åº¦ï¼‰
            outputs = self.model(inputs)  # å‰å‘ä¼ æ’­ï¼šè¾“å…¥â†’æ¨¡å‹â†’è¾“å‡ºé¢„æµ‹å€¼
            loss = self.criterion(outputs, targets)  # è®¡ç®—æŸå¤±ï¼ˆé¢„æµ‹å€¼vsçœŸå®æ ‡ç­¾ï¼‰
            loss.backward()  # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
            self.optimizer.step()  # ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°

            # ç´¯è®¡æŸå¤±ï¼ˆloss.item()è·å–å¼ é‡çš„æ•°å€¼ï¼‰
            running_loss += loss.item()
            # è®¡ç®—å‡†ç¡®ç‡ï¼šoutputs.max(1)å–æ¯ä¸ªæ ·æœ¬é¢„æµ‹æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«
            _, predicted = outputs.max(1)
            total += targets.size(0)  # ç´¯è®¡æ€»æ ·æœ¬æ•°
            correct += predicted.eq(targets).sum().item()  # ç´¯è®¡æ­£ç¡®æ•°

        # è®¡ç®—æœ¬è½®å¹³å‡æŸå¤±ï¼ˆæ€»æŸå¤±/æ‰¹æ¬¡æ•°é‡ï¼‰
        avg_loss = running_loss / len(self.trainloader)
        # è®¡ç®—æœ¬è½®å‡†ç¡®ç‡ï¼ˆæ­£ç¡®æ•°/æ€»æ•° Ã— 100â†’ç™¾åˆ†æ¯”ï¼‰
        accuracy = 100. * correct / total
        return avg_loss, accuracy

    def evaluate(self):
        """
        æµ‹è¯•é›†è¯„ä¼°ï¼šä¸æ›´æ–°å‚æ•°ï¼Œä»…è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡ï¼Œè®°å½•é¢„æµ‹ç»“æœ
        :return: avg_loss - æµ‹è¯•é›†å¹³å‡æŸå¤±ï¼Œaccuracy - æµ‹è¯•é›†å‡†ç¡®ç‡ï¼Œpreds - æ‰€æœ‰é¢„æµ‹æ ‡ç­¾ï¼Œtargets - æ‰€æœ‰çœŸå®æ ‡ç­¾
        """
        self.model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨Dropoutã€BatchNormå›ºå®šï¼‰
        test_loss = 0.0
        correct = 0
        total = 0
        all_preds = []  # ä¿å­˜æ‰€æœ‰é¢„æµ‹æ ‡ç­¾
        all_targets = []  # ä¿å­˜æ‰€æœ‰çœŸå®æ ‡ç­¾

        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆåŠ é€Ÿï¼ŒèŠ‚çœå†…å­˜ï¼‰
            # éå†æµ‹è¯•é›†æ‰¹æ¬¡
            for inputs, targets in self.testloader:
                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)
                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)

                test_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

                # ä¿å­˜é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ï¼ˆè½¬CPUâ†’numpyï¼Œä¾¿äºåç»­è®¡ç®—æ··æ·†çŸ©é˜µï¼‰
                all_preds.extend(predicted.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())

        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_loss = test_loss / len(self.testloader)
        accuracy = 100. * correct / total
        self.final_test_acc = accuracy  # ä¿å­˜æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡
        return avg_loss, accuracy, np.array(all_preds), np.array(all_targets)

    def train(self, epochs=15, patience=5):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹ï¼šå¤šè½®è®­ç»ƒ+æ—©åœ+å­¦ä¹ ç‡è°ƒåº¦+è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
        :param epochs: æœ€å¤§è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤15
        :param patience: æ—©åœè€å¿ƒå€¼ï¼ˆè¿ç»­patienceè½®æ— æå‡åˆ™åœæ­¢ï¼‰ï¼Œé»˜è®¤5
        :return: è®­ç»ƒ/æµ‹è¯•çš„æŸå¤±å’Œå‡†ç¡®ç‡åˆ—è¡¨ï¼ˆç”¨äºç»˜å›¾ï¼‰
        """
        # åˆå§‹åŒ–åˆ—è¡¨ï¼šè®°å½•æ¯è½®çš„æŸå¤±å’Œå‡†ç¡®ç‡
        train_losses, train_accs, test_losses, test_accs = [], [], [], []
        early_stop_counter = 0  # æ—©åœè®¡æ•°å™¨ï¼ˆè¿ç»­æ— æå‡çš„è½®æ•°ï¼‰

        print("\n========== å¼€å§‹è®­ç»ƒ ==========")
        # éå†æ¯ä¸€è½®è®­ç»ƒ
        for epoch in range(epochs):
            # å•è½®è®­ç»ƒï¼šè¿”å›è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡
            train_loss, train_acc = self.train_epoch()
            # å•è½®è¯„ä¼°ï¼šè¿”å›æµ‹è¯•æŸå¤±å’Œå‡†ç¡®ç‡
            test_loss, test_acc, _, _ = self.evaluate()

            # è®°å½•æœ¬è½®ç»“æœ
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            test_losses.append(test_loss)
            test_accs.append(test_acc)

            # æ‰“å°æœ¬è½®è®­ç»ƒ/æµ‹è¯•ç»“æœï¼ˆæ ¼å¼åŒ–è¾“å‡ºï¼Œå¯¹é½æ›´ç¾è§‚ï¼‰
            print(f'Epoch {epoch + 1:2d}/{epochs} | è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%')
            print(f'{"":12} | æµ‹è¯•æŸå¤±: {test_loss:.4f} | æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%\n')

            # å­¦ä¹ ç‡è°ƒåº¦ï¼šæ¯è½®æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()

            # æ—©åœæœºåˆ¶ï¼šåˆ¤æ–­æ˜¯å¦æ›´æ–°æœ€ä½³æ¨¡å‹
            if test_acc > self.best_accuracy:
                self.best_accuracy = test_acc  # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
                self.save_model('best_model.pth')  # ä¿å­˜æœ€ä¼˜æ¨¡å‹
                early_stop_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
            else:
                early_stop_counter += 1  # è®¡æ•°å™¨+1
                # è‹¥è¿ç»­patienceè½®æ— æå‡ï¼Œè§¦å‘æ—©åœ
                if early_stop_counter >= patience:
                    print(f"æ—©åœè§¦å‘ï¼åœ¨ç¬¬ {epoch + 1} è½®åœæ­¢è®­ç»ƒï¼ˆæœ€ä½³å‡†ç¡®ç‡: {self.best_accuracy:.2f}%ï¼‰")
                    break  # é€€å‡ºè®­ç»ƒå¾ªç¯

        # ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹æ›²çº¿ï¼ˆæŸå¤±+å‡†ç¡®ç‡ï¼‰
        self.plot_training_curves(train_losses, train_accs, test_losses, test_accs)
        return train_losses, train_accs, test_losses, test_accs

    def plot_training_curves(self, train_losses, train_accs, test_losses, test_accs):
        """
        ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹çš„æŸå¤±å’Œå‡†ç¡®ç‡æ›²çº¿å›¾ï¼ˆä¿å­˜å›¾ç‰‡ï¼‰
        ä½œç”¨ï¼šç›´è§‚æŸ¥çœ‹æ¨¡å‹è®­ç»ƒè¶‹åŠ¿ï¼ˆæ˜¯å¦è¿‡æ‹Ÿåˆã€æ”¶æ•›æƒ…å†µï¼‰
        :param train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        :param train_accs: è®­ç»ƒå‡†ç¡®ç‡åˆ—è¡¨
        :param test_losses: æµ‹è¯•æŸå¤±åˆ—è¡¨
        :param test_accs: æµ‹è¯•å‡†ç¡®ç‡åˆ—è¡¨
        """
        # åˆ›å»º1è¡Œ2åˆ—çš„å­å›¾ï¼ˆæŸå¤±æ›²çº¿+å‡†ç¡®ç‡æ›²çº¿ï¼‰
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('æ¨¡å‹è®­ç»ƒè¿‡ç¨‹', fontsize=16)

        # å­å›¾1ï¼šæŸå¤±æ›²çº¿
        ax1.plot(train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)  # è“è‰²çº¿ï¼šè®­ç»ƒæŸå¤±
        ax1.plot(test_losses, 'r-', label='æµ‹è¯•æŸå¤±', linewidth=2)  # çº¢è‰²çº¿ï¼šæµ‹è¯•æŸå¤±
        ax1.set_xlabel('è®­ç»ƒè½®æ•°', fontsize=12)
        ax1.set_ylabel('æŸå¤±å€¼', fontsize=12)
        ax1.set_title('è®­ç»ƒ/æµ‹è¯•æŸå¤±å˜åŒ–', fontsize=14)
        ax1.legend(fontsize=10)  # å›¾ä¾‹
        ax1.grid(True, alpha=0.3)  # ç½‘æ ¼ï¼ˆé€æ˜åº¦0.3ï¼Œæ›´ç¾è§‚ï¼‰

        # å­å›¾2ï¼šå‡†ç¡®ç‡æ›²çº¿
        ax2.plot(train_accs, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)  # è“è‰²çº¿ï¼šè®­ç»ƒå‡†ç¡®ç‡
        ax2.plot(test_accs, 'r-', label='æµ‹è¯•å‡†ç¡®ç‡', linewidth=2)  # çº¢è‰²çº¿ï¼šæµ‹è¯•å‡†ç¡®ç‡
        ax2.set_xlabel('è®­ç»ƒè½®æ•°', fontsize=12)
        ax2.set_ylabel('å‡†ç¡®ç‡(%)', fontsize=12)
        ax2.set_title('è®­ç»ƒ/æµ‹è¯•å‡†ç¡®ç‡å˜åŒ–', fontsize=14)
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        # ä¿å­˜å›¾ç‰‡
        plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')
        plt.close()
        print("âœ… å·²ä¿å­˜è®­ç»ƒæŸå¤±/å‡†ç¡®ç‡æ›²çº¿å›¾: training_curves.png")

    def save_model(self, path):
        """
        ä¿å­˜æ¨¡å‹æƒé‡ï¼ˆå«æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨å‚æ•°ã€æœ€ä½³å‡†ç¡®ç‡ï¼‰
        :param path: ä¿å­˜è·¯å¾„ï¼ˆå¦‚'best_model.pth'ï¼‰
        """
        torch.save({
            'model_state_dict': self.model.state_dict(),  # æ¨¡å‹å‚æ•°
            'optimizer_state_dict': self.optimizer.state_dict(),  # ä¼˜åŒ–å™¨å‚æ•°
            'best_accuracy': self.best_accuracy  # æœ€ä½³å‡†ç¡®ç‡ï¼ˆä¾¿äºåç»­æŸ¥çœ‹ï¼‰
        }, path)
        print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {path} (å‡†ç¡®ç‡: {self.best_accuracy:.2f}%)")

    def analyze_confusion_matrix(self, classes):
        """
        è®¡ç®—å¹¶åˆ†ææ··æ·†çŸ©é˜µï¼š
        1. ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
        2. è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡
        3. åˆ†æç±»åˆ«æ··æ·†æƒ…å†µ
        4. è¾“å‡ºå®Œæ•´åˆ†ç±»æŠ¥å‘Š
        :param classes: ç±»åˆ«åç§°åˆ—è¡¨ï¼ˆä¸­æ–‡ï¼‰
        :return: cm - æ··æ·†çŸ©é˜µï¼Œclass_report - åˆ†ç±»æŠ¥å‘Šå­—å…¸
        """
        # è·å–æµ‹è¯•é›†çš„é¢„æµ‹æ ‡ç­¾å’ŒçœŸå®æ ‡ç­¾
        _, _, preds, targets = self.evaluate()

        # 1. è®¡ç®—æ··æ·†çŸ©é˜µï¼šè¡Œ=çœŸå®ç±»åˆ«ï¼Œåˆ—=é¢„æµ‹ç±»åˆ«
        cm = confusion_matrix(targets, preds)

        # 2. ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
        plt.figure(figsize=(12, 10))
        # sns.heatmapï¼šç»˜åˆ¶çƒ­åŠ›å›¾ï¼Œannot=True=æ˜¾ç¤ºæ•°å€¼ï¼Œfmt='d'=æ•´æ•°æ ¼å¼ï¼Œcmap='Blues'=è“è‰²ç³»
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=classes, yticklabels=classes,
                    annot_kws={"size": 10})  # æ•°å€¼å­—ä½“å¤§å°
        plt.xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
        plt.ylabel('çœŸå®ç±»åˆ«', fontsize=12)
        # æ ‡é¢˜ï¼šåŒ…å«æµ‹è¯•é›†æ•´ä½“å‡†ç¡®ç‡
        plt.title(f'Fashion-MNIST æ··æ·†çŸ©é˜µ (æµ‹è¯•é›†å‡†ç¡®ç‡: {self.final_test_acc:.2f}%)', fontsize=14)
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
        plt.close()

        # 3. è¯¦ç»†åˆ†ææ··æ·†æƒ…å†µ
        print("\n========== æ··æ·†çŸ©é˜µåˆ†æ ==========")
        print(f"ğŸ“Š æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ•´ä½“å‡†ç¡®ç‡: {self.final_test_acc:.2f}%")
        print("\nğŸ” ç±»åˆ«æ··æ·†è¯¦æƒ…:")
        print("-" * 60)

        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šï¼ˆåŒ…å«ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ï¼‰
        class_report = classification_report(targets, preds, target_names=classes, output_dict=True)
        # éå†æ¯ä¸ªç±»åˆ«ï¼Œåˆ†æè¯¦ç»†æŒ‡æ ‡
        for i, cls in enumerate(classes):
            correct = cm[i, i]  # çœŸå®ä¸ºiä¸”é¢„æµ‹ä¸ºiçš„æ•°é‡ï¼ˆæ­£ç¡®æ•°ï¼‰
            total_true = cm[i, :].sum()  # çœŸå®ä¸ºiçš„æ€»æ•°
            total_pred = cm[:, i].sum()  # é¢„æµ‹ä¸ºiçš„æ€»æ•°

            # ç±»åˆ«å‡†ç¡®ç‡ï¼ˆå¬å›ç‡ï¼‰ï¼šæ­£ç¡®æ•°/çœŸå®æ€»æ•°
            acc = 100. * correct / total_true if total_true > 0 else 0
            # ç²¾ç¡®ç‡ï¼šæ­£ç¡®æ•°/é¢„æµ‹æ€»æ•°
            precision = 100. * correct / total_pred if total_pred > 0 else 0

            # æ‰“å°å½“å‰ç±»åˆ«çš„æŒ‡æ ‡
            print(f"ç±»åˆ« {i:2d} [{cls}]:")
            print(f"  - å‡†ç¡®ç‡(å¬å›ç‡): {acc:.2f}% ({correct}/{total_true})")
            print(f"  - ç²¾ç¡®ç‡: {precision:.2f}% ({correct}/{total_pred})")

            # æ‰¾å‡ºå½“å‰ç±»åˆ«æœ€æ˜“æ··æ·†çš„ç±»åˆ«
            cm_copy = cm[i].copy()
            cm_copy[i] = 0  # æ’é™¤è‡ªèº«ï¼ˆåªçœ‹æ··æ·†çš„ç±»åˆ«ï¼‰
            max_confuse_idx = np.argmax(cm_copy)  # æ··æ·†æ•°æœ€å¤šçš„ç±»åˆ«ç´¢å¼•
            max_confuse_count = cm_copy[max_confuse_idx]  # æ··æ·†æ•°é‡
            if max_confuse_count > 0:
                print(f"  - æœ€æ˜“æ··æ·†ä¸º: {classes[max_confuse_idx]} (æ•°é‡: {max_confuse_count})")
            print()  # ç©ºè¡Œåˆ†éš”

        # è¾“å‡ºå®Œæ•´åˆ†ç±»æŠ¥å‘Šï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
        print("ğŸ“‹ å®Œæ•´åˆ†ç±»æŠ¥å‘Š:")
        print("-" * 60)
        print(classification_report(targets, preds, target_names=classes, digits=2))

        return cm, class_report


class ImagePredictor:

    def __init__(self, model, classes):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        :param model: è®­ç»ƒå¥½çš„CNNæ¨¡å‹
        :param classes: ç±»åˆ«åç§°åˆ—è¡¨
        """
        self.model = model.to(DEVICE)  # æ¨¡å‹ç§»åˆ°æŒ‡å®šè®¾å¤‡
        self.model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        self.classes = classes  # ç±»åˆ«åç§°

    def preprocess_image(self, image_path):
        """
        é¢„å¤„ç†è‡ªå®šä¹‰å›¾ç‰‡ï¼ˆåŒ¹é…æ¨¡å‹è¾“å…¥æ ¼å¼ï¼‰
        :param image_path: å›¾ç‰‡è·¯å¾„
        :return: é¢„å¤„ç†åçš„å¼ é‡ï¼ˆbatch_size Ã— 1 Ã— 28 Ã— 28ï¼‰ï¼Œå¤±è´¥è¿”å›None
        """
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            return None

        try:
            # 1. æ‰“å¼€å›¾ç‰‡å¹¶è½¬æ¢ä¸ºç°åº¦å›¾ï¼ˆåŒ¹é…Fashion-MNISTæ ¼å¼ï¼‰
            image = Image.open(image_path).convert('L')
            # 2. åè½¬ç°åº¦ï¼šFashion-MNISTæ˜¯ç™½åº•é»‘å›¾ï¼Œå¤–éƒ¨å›¾ç‰‡é€šå¸¸æ˜¯é»‘åº•ç™½å›¾ï¼Œåè½¬ååŒ¹é…
            image = ImageOps.invert(image)

            # 3. é¢„å¤„ç†æµæ°´çº¿ï¼ˆä¸è®­ç»ƒé›†ä¸€è‡´ï¼‰
            preprocess = transforms.Compose([
                transforms.Resize((32, 32)),  # å…ˆæ”¾å¤§åˆ°32Ã—32ï¼ˆé¿å…ç›´æ¥ç¼©28Ã—28å˜å½¢ï¼‰
                transforms.CenterCrop((28, 28)),  # ä¸­å¿ƒè£å‰ªåˆ°28Ã—28
                transforms.ToTensor(),  # è½¬å¼ é‡
                transforms.Normalize((0.5,), (0.5,))  # æ ‡å‡†åŒ–ï¼ˆä¸è®­ç»ƒé›†ä¸€è‡´ï¼‰
            ])

            # 4. åº”ç”¨é¢„å¤„ç†å¹¶æ·»åŠ batchç»´åº¦ï¼ˆæ¨¡å‹è¦æ±‚è¾“å…¥æ˜¯æ‰¹é‡ï¼Œå³ä½¿å•å¼ å›¾ç‰‡ï¼‰
            tensor_image = preprocess(image).unsqueeze(0)
            return tensor_image

        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼ˆå¦‚å›¾ç‰‡æ ¼å¼é”™è¯¯ã€è·¯å¾„é”™è¯¯ç­‰ï¼‰
            print(f"âŒ å›¾ç‰‡å¤„ç†å¤±è´¥: {e}")
            return None

    def predict_image(self, image_tensor):
        """
        é¢„æµ‹è‡ªå®šä¹‰å›¾ç‰‡
        :param image_tensor: é¢„å¤„ç†åçš„å¼ é‡
        :return: predicted_class - é¢„æµ‹ç±»åˆ«åç§°ï¼Œconfidence_score - ç½®ä¿¡åº¦ï¼ˆç™¾åˆ†æ¯”ï¼‰
        """
        if image_tensor is None:
            return None, 0.0

        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            # å‰å‘ä¼ æ’­ï¼šè¾“å…¥â†’æ¨¡å‹â†’è¾“å‡ºé¢„æµ‹å€¼
            outputs = self.model(image_tensor.to(DEVICE))
            # è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆsoftmaxå½’ä¸€åŒ–ï¼Œä½¿æ‰€æœ‰ç±»åˆ«æ¦‚ç‡å’Œä¸º1ï¼‰
            probs = F.softmax(outputs, dim=1)
            # è·å–æœ€å¤§æ¦‚ç‡çš„ç½®ä¿¡åº¦å’Œç±»åˆ«ç´¢å¼•
            confidence, predicted = torch.max(probs, 1)

        # è½¬æ¢ä¸ºç±»åˆ«åç§°å’Œç½®ä¿¡åº¦ç™¾åˆ†æ¯”
        predicted_class = self.classes[predicted.item()]
        confidence_score = confidence.item() * 100

        # æ‰“å°é¢„æµ‹ç»“æœ
        print("\n========== è‡ªå®šä¹‰å›¾ç‰‡é¢„æµ‹ç»“æœ ==========")
        print(f"é¢„æµ‹ç±»åˆ«: {predicted_class}")
        print(f"ç½®ä¿¡åº¦: {confidence_score:.2f}%")
        print("\næ‰€æœ‰ç±»åˆ«ç½®ä¿¡åº¦ï¼ˆé™åºï¼‰:")

        # å°†æ¦‚ç‡è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œä¾¿äºæ’åº
        prob_list = probs.squeeze().cpu().numpy()
        # æŒ‰æ¦‚ç‡ä»é«˜åˆ°ä½æ’åºï¼Œè·å–ç´¢å¼•
        sorted_indices = np.argsort(prob_list)[::-1]
        # éå†æ’åºåçš„ç´¢å¼•ï¼Œæ‰“å°æ¯ä¸ªç±»åˆ«çš„ç½®ä¿¡åº¦
        for idx in sorted_indices:
            print(f"  {self.classes[idx]:<8}: {prob_list[idx] * 100:.2f}%")

        return predicted_class, confidence_score


if __name__ == "__main__":
    """
    ä¸»ç¨‹åºæµç¨‹ï¼š
    1. æ•°æ®å‡†å¤‡ â†’ 2. æ¨¡å‹å®šä¹‰ä¸ç»“æ„å¯è§†åŒ– â†’ 3. æ¨¡å‹è®­ç»ƒ â†’ 4. æ··æ·†çŸ©é˜µåˆ†æ â†’ 5. è‡ªå®šä¹‰å›¾ç‰‡é¢„æµ‹
    """
    # 1. æ•°æ®å‡†å¤‡ä¸éšæœºæ ·æœ¬å¯è§†åŒ–
    data_processor = DataProcessor(batch_size=64)  # åˆ›å»ºæ•°æ®å¤„ç†å™¨ï¼ˆæ‰¹é‡64ï¼‰
    trainloader, testloader = data_processor.load_data()  # åŠ è½½æ•°æ®
    data_processor.visualize_random_samples()  # å¯è§†åŒ–æ¯ä¸ªç±»åˆ«æ ·æœ¬

    # 2. æ¨¡å‹å®šä¹‰ä¸ç»“æ„æè¿°
    model = FashionCNN()  # åˆ›å»ºCNNæ¨¡å‹
    print("\n========== æ¨¡å‹ç»“æ„æè¿° ==========")
    # æ–‡å­—æè¿°æ¨¡å‹ç»“æ„ï¼ˆä¾¿äºç†è§£ï¼‰
    print("ğŸ“Œ FashionCNN æ¨¡å‹ç»“æ„:")
    print("  è¾“å…¥: 1Ã—28Ã—28 ç°åº¦å›¾åƒ")
    print("  å·ç§¯å—1: Conv2d(32) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ è¾“å‡º32Ã—14Ã—14")
    print("  å·ç§¯å—2: Conv2d(64) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ è¾“å‡º64Ã—7Ã—7")
    print("  å·ç§¯å—3: Conv2d(128) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ è¾“å‡º128Ã—3Ã—3")
    print("  å…¨è¿æ¥å±‚: 128Ã—3Ã—3 â†’ 256 â†’ Dropout(0.3) â†’ 128 â†’ Dropout(0.3) â†’ 10ï¼ˆè¾“å‡ºï¼‰")

    # å›¾è¡¨å½¢å¼ï¼šä½¿ç”¨torchsummaryè¾“å‡ºå±‚ç»´åº¦å’Œå‚æ•°ï¼ˆç›´è§‚å±•ç¤ºæ¨¡å‹ç»“æ„ï¼‰
    print("\nğŸ“Œ æ¨¡å‹å±‚ç»´åº¦è¯¦æƒ…:")
    summary(model, input_size=(1, 28, 28))  # è¾“å…¥å°ºå¯¸ï¼š1Ã—28Ã—28ï¼ˆç°åº¦å›¾ï¼‰

    # 3. æ¨¡å‹è®­ç»ƒ
    trainer = ModelTrainer(model, trainloader, testloader)  # åˆ›å»ºè®­ç»ƒå™¨
    trainer.train(epochs=15, patience=5)  # å¼€å§‹è®­ç»ƒï¼ˆ15è½®ï¼Œæ—©åœè€å¿ƒå€¼5ï¼‰

    # 4. æ··æ·†çŸ©é˜µåˆ†æ + æµ‹è¯•é›†å‡†ç¡®ç‡è¾“å‡º
    trainer.analyze_confusion_matrix(data_processor.classes)

    # 5. è‡ªå®šä¹‰å›¾ç‰‡é¢„æµ‹
    predictor = ImagePredictor(model, data_processor.classes)  # åˆ›å»ºé¢„æµ‹å™¨
    image_path = "111.jpg"  # è‡ªå®šä¹‰å›¾ç‰‡è·¯å¾„ï¼ˆéœ€æ”¾åœ¨è„šæœ¬åŒç›®å½•ï¼‰
    if os.path.exists(image_path):
        processed_img = predictor.preprocess_image(image_path)  # é¢„å¤„ç†å›¾ç‰‡
        if processed_img is not None:
            predictor.predict_image(processed_img)  # é¢„æµ‹å›¾ç‰‡
    else:
        print(f"\nâš ï¸  è‡ªå®šä¹‰å›¾ç‰‡ {image_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡é¢„æµ‹")

    # æ‰“å°å®Œæˆæç¤ºï¼ˆåˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶ï¼‰
    print("\nğŸ‰ æ‰€æœ‰åˆ†æä»»åŠ¡å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - random_class_samples.png (æ¯ä¸ªç±»åˆ«éšæœºæ ·æœ¬)")
    print("  - training_curves.png (æŸå¤±/å‡†ç¡®ç‡æ›²çº¿)")
    print("  - confusion_matrix.png (æ··æ·†çŸ©é˜µ)")
    print("  - best_model.pth (æœ€ä½³æ¨¡å‹æƒé‡)")